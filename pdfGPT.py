from dotenv import load_dotenv
import faiss
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import os
import pickle
from pdfminer.high_level import extract_text
from transformers import GPT2LMHeadModel, GPT2TokenizerFast


# Parser to handle the model argument.
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('-f', '--file',
                        type=str,
                        required=True,
                        help=f'The pdf file you want to chat with. Only provide the name of the file that is placed in the pdfs/ directory.')

    return parser.parse_args().model


class Pdf:
    def __init__(self, pdf_file):
        load_dotenv()
        # Checks for openAI API key.
        if os.environ.get('OPENAI_API_KEY') is None:
            print("Please provide a valid OpenAI API key in the .env file. See .env.example for more information")
            exit(1)
        self.filename = pdf_file.removesuffix()(".pdf")
        self.pdf_file = f"pdfs/{pdf_file}"
        self.txt_file = f"txts/{self.filename}.txt"
        self.db_name = f"dbs/{self.filename}.pkl"
        self.chunks_path = f"chunks/{pdf_file}_chunks.index"

        if not os.path.isfile(self.txt_file):
            print(f"Creating {self.txt_file}...")
            self.parse_pdf()
        if not os.path.isfile(self.db_name):
            print(f"Splitting {self.txt_file}...")
            chunks = self.split_txt()
            print(f"Creating {self.db_name}...")
            self.create_faiss_db(chunks)

        self.load_faiss_db()
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        self.qa_chain = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.05), self.db.as_retriever(), memory=memory)


    def run(self):
        print("Starting chat. Type 'q' or 'exit' to quit.")
        while True:
            query = input(f"Chatting with {self.filename}.pdf. Ask your question: ")
            if not query:
                continue
            if query == "q" or query == "exit":
                return
            res = self.search(query)
            print(res)
            query = ""


    # Converts pdf file to txt file.
    def parse_pdf(self):
        text = extract_text(self.pdf_file)
        with open(self.txt_file, 'w+') as f:
            f.write(text)


    # Splits txt file into chunks.
    def split_txt(self):
        with open(self.txt_file, "r") as f:
            text = f.read()

        text_splitter = CharacterTextSplitter(chunk_size=1250, separator="\n\n")
        chunks = []
        splits = text_splitter.split_text(text)
        chunks.extend(splits)

        return chunks


    # Creates and saves Faiss db from the chunks generated by split_txt().
    def create_faiss_db(self, chunks):
        store = FAISS.from_texts(chunks, OpenAIEmbeddings())
        faiss.write_index(store.index, self.chunks_path)
        store.index = None
        with open(self.db_name, "wb") as f:
            pickle.dump(store, f)


    # Loads Faiss db into memory.
    def load_faiss_db(self):
        index = faiss.read_index(self.chunks_path)

        with open(self.db_name, "rb") as f:
            db = pickle.load(f)

        db.index = index
        self.db = db


    # Searches for the query in the Faiss db.
    def search(self, query):
        result = self.qa_chain({"question": query})
        return result["answer"]


pdf_file = parse_args()
chatter = Pdf(pdf_file=pdf_file)
chatter.run()
